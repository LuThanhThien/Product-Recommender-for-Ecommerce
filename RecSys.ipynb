{"cells":[{"cell_type":"markdown","metadata":{},"source":["# Libraries"]},{"cell_type":"code","execution_count":1,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2023-08-22T08:47:51.466734Z","iopub.status.busy":"2023-08-22T08:47:51.466250Z","iopub.status.idle":"2023-08-22T08:47:51.482691Z","shell.execute_reply":"2023-08-22T08:47:51.481271Z","shell.execute_reply.started":"2023-08-22T08:47:51.466694Z"},"trusted":true},"outputs":[],"source":["import numpy as np \n","import pandas as pd \n","import matplotlib.pyplot as plt\n","import tensorflow as tf\n","import os, random\n","import plotly.express as px\n","import seaborn as sns\n","from sklearn.preprocessing import LabelEncoder\n","from wordcloud import WordCloud\n","import time\n","from tensorflow import keras\n","from sklearn.preprocessing import StandardScaler, MinMaxScaler\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import LabelEncoder\n","from sklearn.linear_model import SGDRegressor\n","from sklearn.tree import DecisionTreeRegressor\n","from sklearn.ensemble import RandomForestRegressor\n","import sklearn.metrics \n","from tensorflow import keras \n","from tensorflow.keras import optimizers\n","from tensorflow.keras.callbacks import ModelCheckpoint, TensorBoard\n","from tensorflow.keras.models import Model\n","from tensorflow.keras.layers import Input\n","from modules.DataProcessing import *\n","from itertools import product # custom grid search\n","import random \n","import sys\n","\n","input_path = 'inputs\\\\data\\\\comestic'\n","input_clean = 'outputs\\\\data\\\\comestic'\n","name = '2019-Dec'"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[],"source":["gpus = tf.config.experimental.list_physical_devices('GPU')\n","for gpu in gpus:\n","    tf.config.experimental.set_memory_growth(gpu, True)"]},{"cell_type":"markdown","metadata":{},"source":["# Recommendation System \n","\n","## 1. Interaction Matrix\n","\n","In collaborative filtering, it will based on the past interaction of user and of similar users to give recommended items. We need 3 main matrix:\n","\n","1. User matrix: containing user content\n","\n","2. Product matrix: contains product information\n","\n","3. Interation matrix: display the interaction score (e.g. ratings) of all users toward every available products. \n","\n","Alternating Least Squares algorithm is one approach to collaborative filtering that can be used to train a model on this interaction matrix. We will derive the interaction matrix based on recency combined with weight decay. Recent interactions are often more indicative of a user's current preferences than older interactions. In this case, we will intialize the interval recency for interaction matrix as 15 days (half-month) and weight decay to be 0.5 for this interval."]},{"cell_type":"code","execution_count":124,"metadata":{},"outputs":[],"source":["df = pd.read_csv(f'{input_clean}\\\\{name}-feature.csv').sort_values(by='user_id')"]},{"cell_type":"code","execution_count":125,"metadata":{},"outputs":[],"source":["cleandata = pd.read_csv(f'{input_clean}\\\\{name}-clean.csv')"]},{"cell_type":"code","execution_count":126,"metadata":{},"outputs":[],"source":["cleaner = DataCleaner(cleandata)"]},{"cell_type":"code","execution_count":127,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["CREATING USER TABLE...\n","Basic features processing...\n","Interaction features processing...\n","Calculating relative price...\n","End calculating relative price. Finished in 5.062s.\n","-------------------------------------------------------------\n","Interaction rates processing...\n","Check datatypes processing...\n","Create user table successfully. Finished in 56.073s.\n","-------------------------------------------------------------\n","CREATING PRODUCT TABLE...\n","Basic features processing...\n","Interaction rates processing...\n","Create product table successfully. Finished in 25.803s.\n","-------------------------------------------------------------\n"]}],"source":["userTable = cleaner.create_user_table(is_return=True)\n","productTable = cleaner.create_product_table(is_return=True)"]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[],"source":["def checkpoint_call(model_name=None):\n","    if model_name==None:\n","        raise NameError('Need to specify model name.')\n","    \n","    checkpoint_path = 'outputs\\\\checkpoints'\n","    checkpoint_path = checkpoint_path + f'\\\\{model_name}'\n","        \n","    if not os.path.exists(f'{checkpoint_path}'):\n","        os.makedirs(f'{checkpoint_path}')\n","        \n","    model_ver = max([int(i) for i in os.listdir(f\"{checkpoint_path}\")]+[0]) + 1\n","    filepath=f\"{checkpoint_path}\\\\{model_ver}\\\\\"+\"{epoch}.ckpt\"\n","\n","    return ModelCheckpoint(filepath, monitor='val_loss', verbose=1,\n","                        save_weights_only=True, save_best_only=True, mode='min')"]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[],"source":["def model_setup(model_name=None, learning_rate=0.01, lr_schedule=False, optimizer='adam', callback=True):\n","\n","    setup_params = {\n","        'optimizer': None,\n","        'loss':  tf.keras.losses.MeanSquaredError(),\n","        'callbacks': None\n","    }\n","\n","    # learning rate\n","    initial_learning_rate = learning_rate\n","    lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n","        initial_learning_rate,\n","        decay_steps=100,\n","        decay_rate=0.96,\n","        staircase=True)\n","    \n","    if lr_schedule:\n","        learning_rate = lr_schedule\n","    else: learning_rate = learning_rate\n","\n","    # optimizer\n","    optimizers = {\n","        'adam': tf.keras.optimizers.Adam(learning_rate=learning_rate, decay=1e-6),\n","        'rmsprop': tf.keras.optimizers.RMSprop(learning_rate=learning_rate),\n","        'sgd': tf.keras.optimizers.SGD(learning_rate=learning_rate)\n","    }\n","\n","    # callback \n","    logdir = 'logs'\n","    callbacks = {\n","        True: [TensorBoard(log_dir=logdir), checkpoint_call(model_name=model_name)],\n","        False: None,\n","    }\n","\n","    setup_params['optimizer'] = optimizers[optimizer]\n","    setup_params['callbacks'] = callbacks[callback]\n","\n","    return setup_params"]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[],"source":["def evaluate_model(y_true, y_pred):\n","    mae = sklearn.metrics.mean_absolute_error(y_true, y_pred)\n","    mse = sklearn.metrics.mean_squared_error(y_true, y_pred)\n","    rmse = np.sqrt(sklearn.metrics.mean_squared_error(y_true, y_pred))\n","    return mae, rmse"]},{"cell_type":"markdown","metadata":{},"source":["## 3.2. Content-based by Neural Network"]},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["User size: 16, product size: 11\n"]}],"source":["# dicts and constants\n","\n","# attributes\n","attributes =['interaction_score', 'user_id', 'views_user', 'carts_user', \n","            'remove_from_carts_user', 'purchases_user', 'avg_view_price',\n","            'avg_purchase_price', 'avg_view_relative_price',\n","            'avg_purchase_relative_price', 'distinct_view_product',\n","            'distinct_cart_product', 'distinct_remove_product',\n","            'distinct_purchase_product', 'cart_per_view_user',\n","            'purchase_per_view_user', 'remove_per_cart_user',\n","            'purchase_per_cart_user', \n","            'product_id', 'category_id', 'avg_price', 'relative_price', 'views_product',\n","            'carts_product', 'remove_from_carts_product', 'purchases_product',\n","            'cart_per_view_product', 'purchase_per_view_product',\n","            'remove_per_cart_product', 'purchase_per_cart_product']\n","\n","# user and product attributes\n","excludes = ['interaction_score', 'user_id', 'product_id']\n","user_attr = [attr for attr in attributes[1:18] if attr not in excludes]\n","product_attr = [attr for attr in attributes[18:] if attr not in excludes]\n","target = 'interaction_score'\n","scaled_attr = [attr for attr in attributes if attr not in excludes]\n","\n","user_size = len(user_attr)\n","product_size = len(product_attr)\n","print('User size: {}, product size: {}'.format(user_size, product_size))"]},{"cell_type":"code","execution_count":11,"metadata":{},"outputs":[],"source":["def overlap_split(df,train_size=0.9):\n","    # train test split\n","    test_size=0.5\n","    train, test = train_test_split(df, train_size=train_size, random_state=42)\n","    dev, test = train_test_split(test, test_size=test_size, random_state=42)\n","    return train, dev, test\n","\n","def disjoint_split(df,train_size=0.9):\n","    test_size=0.5\n","    \n","    # Group data by user_id\n","    user_grouped = df.groupby('user_id')\n","    unique_users = list(user_grouped.groups.keys())\n","\n","    # Shuffle the order of user IDs\n","    random.shuffle(unique_users)\n","\n","    # Calculate the number of users for each split\n","    num_users = len(unique_users)\n","    num_train_users = int(train_size * num_users)\n","    num_dev_users = int((1-train_size) * num_users)\n","\n","    # Split user IDs into train, dev, and test sets\n","    train_users = unique_users[:num_train_users]\n","    dev_users = unique_users[num_train_users:]\n","\n","    # Filter data based on split user IDs\n","    train = df[df['user_id'].isin(train_users)]\n","    dev = df[df['user_id'].isin(dev_users)]\n","    dev = dev.sample(frac = 1)\n","    dev, test = train_test_split(dev, test_size=test_size, random_state=42)\n","\n","    # Verify user distribution in dev and test sets\n","    # common_dev_users = set(dev_users).intersection(train_users)\n","    # if not common_dev_users:\n","    #     print(\"Test and Train users are disjoint.\")\n","    # else: print(\"Test and Train users are overlap!!!\")\n","\n","    return train, dev, test"]},{"cell_type":"code","execution_count":12,"metadata":{},"outputs":[],"source":["def dataScaler(df):\n","\n","    print('Scaling data processing...')\n","    scaler = StandardScaler()\n","    df_scaled = df.copy()\n","    scaler.fit(df[scaled_attr])\n","    df_scaled[scaled_attr] = scaler.fit_transform(df[scaled_attr])\n","    print('Scaled successfully.')\n","    # print(df_scaled.describe())\n","\n","    return df_scaled"]},{"cell_type":"code","execution_count":13,"metadata":{},"outputs":[],"source":["def ModelPipeline(df, models, split_type='disjoint_split', train_size=0.99, epochs=3, batch_size=256,\n","                   learning_rate=0.01, lr_schedule=False, callback=True):\n","\n","    split_types = {\n","        'disjoint_split': disjoint_split,\n","        'overlap_split': overlap_split,\n","    }\n","\n","    #df_scaled = dataScaler(df)\n","    df_scaled = df\n","    \n","    print('Train-Dev-Test plitting proceesing... Split type:', split_type)\n","    train, dev, test = split_types[split_type](df_scaled, train_size=train_size)\n","\n","    print('Found {} model(s)'.format(len(models)))\n","    print(models.keys())\n","\n","    # loop through models\n","    for key, model in zip(models.keys(), models.values()):\n","        print('Running {} model...'.format(key))\n","\n","        start = time.time()\n","        currentmodel = model\n","        \n","        # model summary\n","        num_layers = len(currentmodel.layers)\n","        print(\"Number of layers: {}\".format(num_layers))\n","        currentmodel.summary()\n","\n","        # set up model\n","        setup = model_setup(model_name=key, learning_rate=learning_rate, lr_schedule=lr_schedule, callback=callback)\n","        callbacks = setup['callbacks']\n","        currentmodel.compile(optimizer=setup['optimizer'], loss=setup['loss'], metrics=[tf.keras.metrics.RootMeanSquaredError()])\n","\n","        # train model\n","        print('Training processing...')\n","        hist = currentmodel.fit([train[user_attr], train[product_attr]], train[target],\n","                        validation_data = [[dev[user_attr], dev[product_attr]], dev[target]], \n","                        epochs=epochs, batch_size=batch_size, verbose=1, callbacks=[callbacks])\n","        \n","        # predict\n","        print('Predicting processing...')\n","        preds = currentmodel.predict([test[user_attr], test[product_attr]])\n","\n","        print('Evaluation processing...')\n","        eval_loss = currentmodel.evaluate([test[user_attr], test[product_attr]], test[target])\n","        print(\"Evaluate on test data: {}\".format(eval_loss))\n","\n","        rae, rmae = evaluate_model(test[target], preds)\n","        print('MAE = {:.3f}, RMAE = {:.3f}'.format(rae, rmae))\n","\n","        end = time.time()\n","        print('\\nFinished {} model in {:.3f}s\\n'.format(key, end-start))\n","\n","        "]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":154,"metadata":{},"outputs":[],"source":["# define models\n","def contentNN(content_size=None, neurals=[64, 64, 32], num_outputs=32):\n","    content_NN = tf.keras.models.Sequential()\n","    content_NN.add(tf.keras.layers.Input(shape=(content_size)))\n","    \n","    for neural in neurals:\n","        content_NN.add(tf.keras.layers.BatchNormalization())\n","        content_NN.add(tf.keras.layers.Dense(neural, activation='relu'))\n","    \n","    content_NN.add(tf.keras.layers.Dense(num_outputs, activation='linear'))\n","    \n","    return content_NN\n","\n","\n","def CBDeepLearning(neurals=[64, 64, 32], num_outputs = 32):\n","\n","    tf.random.set_seed(1)\n","    \n","    user_NN = contentNN(user_size, neurals, num_outputs)\n","    product_NN = contentNN(product_size, neurals, num_outputs)\n","\n","    # create the user input and point to the base network\n","    input_user = tf.keras.layers.Input(shape=(user_size))\n","    vu = user_NN(input_user)\n","    vu = tf.linalg.l2_normalize(vu, axis=1)\n","\n","    # create the item input and point to the base network\n","    input_item = tf.keras.layers.Input(shape=(product_size))\n","    vm = product_NN(input_item)\n","    vm = tf.linalg.l2_normalize(vm, axis=1)\n","\n","    # compute the dot product of the two vectors vu and vm\n","    output = tf.keras.layers.Dot(axes=1)([vu, vm])\n","\n","\n","    model = tf.keras.models.Model(inputs=[input_user, input_item], outputs=output)\n","\n","    return model"]},{"cell_type":"code","execution_count":155,"metadata":{},"outputs":[],"source":["num_outputs = 32\n","split_type='disjoint_split'\n","train_size=0.99\n","epochs=5\n","batch_size=1024\n","learning_rate=0.01\n","lr_schedule=False\n","callback=True\n","models = {\n","    'CBDeepLearning': CBDeepLearning(),\n","    }"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# ModelPipeline(df, models=models, split_type=split_type, train_size=train_size, epochs=epochs, batch_size=batch_size, \n","#               learning_rate=learning_rate, lr_schedule=True, callback=True)"]},{"cell_type":"code","execution_count":156,"metadata":{},"outputs":[],"source":["def delaydot(num_dots=3,sleep_time=1, pattern='.'):\n","    for i in range(num_dots):\n","        sys.stdout.write(pattern)\n","        sys.stdout.flush()\n","        time.sleep(sleep_time)\n","    print()"]},{"cell_type":"code","execution_count":157,"metadata":{},"outputs":[],"source":["def printRed(text):\n","    print('\\033[31m' + str(text) + '\\033[0m')\n","\n","def printGreen(text):\n","    print('\\033[32m' + str(text) + '\\033[0m')"]},{"cell_type":"code","execution_count":213,"metadata":{},"outputs":[],"source":["def ModelFineTuning(hyperparams, grid_number=20, train_size=0.99, epochs=3, batch_size=256,\n","                   learning_rate=0.01, lr_schedule=False, callback=True):\n","\n","    results = {\n","        'params': [],\n","        'eval': [],\n","        'checkpoints': [],\n","    }\n","\n","    print('Loading clean data...')\n","    cleandata = pd.read_csv(f'{input_clean}\\\\{name}-clean.csv')\n","    cleaner = DataCleaner(cleandata)\n","    cleaner.relative_price()\n","    cleandata = cleaner.data\n","    cleaner = DataCleaner(cleandata, recency_days=hyperparams['recency_days'], weight_decay=hyperparams['weight_decay'])\n","    df = cleaner.FeatureEngineering(return_merge=True)\n","\n","    # grid search\n","    total_combinations = 1\n","    for values in hyperparams.values():\n","        total_combinations *= len(values)\n","\n","    grid_number = min(total_combinations, grid_number)\n","\n","    print('Running {} random searching'. format(grid_number), end='')\n","    delaydot()\n","\n","    for key in range(grid_number):\n","\n","        params = {k: random.choice(v) for k, v in hyperparams.items()}\n","        while params in results['params']:\n","            params = {k: random.choice(v) for k, v in hyperparams.items()}\n","\n","        print('Fine Tuning Number {}, hyperparameters summary:'.format(key))\n","        printRed(params)\n","        \n","        print('1. Train-Dev-Test plitting proceesing', end='')\n","        delaydot()\n","        train, dev, test = disjoint_split(df, train_size=train_size)\n","\n","        \n","        print('2. Bulding model...')\n","\n","        start = time.time()\n","        currentmodel = CBDeepLearning(neurals=params['neurals'],num_outputs = params['num_outputs'])\n","        \n","        # model summary\n","        num_layers = len(currentmodel.layers)\n","        currentmodel.summary()\n","\n","        # set up model\n","        setup = model_setup(model_name='CBDeepLearning', learning_rate=learning_rate, lr_schedule=lr_schedule, callback=callback)\n","        callbacks = setup['callbacks']\n","        currentmodel.compile(optimizer=setup['optimizer'], loss=setup['loss'], metrics=[tf.keras.metrics.RootMeanSquaredError()])\n","\n","        # train model\n","        print('3. Training processing...')\n","        hist = currentmodel.fit([train[user_attr], train[product_attr]], train[target],\n","                        validation_data = [[dev[user_attr], dev[product_attr]], dev[target]], \n","                        epochs=epochs, batch_size=batch_size, verbose=1, callbacks=[callbacks])\n","        \n","        # predict\n","        print('4. Evaluation processing...')\n","\n","        preds = currentmodel.predict([test[user_attr], test[product_attr]])\n","        eval = currentmodel.evaluate([test[user_attr], test[product_attr]], test[target])\n","        printRed('Evalulation result (RMSE): {}'.format(eval))\n","        print('Saving results...')\n","        \n","        path = f\"outputs//checkpoints//CBDeepLearning\"\n","        checkpoint_ver = max([int(i.split('.')[0]) for i in os.listdir(path)]+[0])\n","        last_checkpoint = path + f\"//{checkpoint_ver}\"\n","        printRed('Saved to {}.'.format(last_checkpoint))\n","\n","        results['params'] = results['params'] + [params]\n","        results['eval'] = results['eval'] + [eval]\n","        results['checkpoints'] = results['checkpoints'] + [last_checkpoint]\n","\n","        end = time.time()\n","        printGreen('\\nFinished model {} in {:.3f}s\\n'.format(key, end-start))\n","        delaydot(num_dots=25, sleep_time=0.05, pattern='-+-')\n","    \n","    best_result = min(results['eval'],  key=lambda x: x[1])\n","    best_id = results['eval'].index(best_result)\n","    best_params = results['params'][best_id]\n","    best_eval = results['eval'][best_id]\n","    best_checkpoint = results['checkpoints'][best_id]\n","    printRed('Best params: ')\n","    printRed(best_params)\n","    printRed('Loss = {}; Checkpoint saved in {}'.format(best_eval, best_checkpoint))\n","\n","    return results\n","        "]},{"cell_type":"code","execution_count":214,"metadata":{},"outputs":[],"source":["def best_params(results):\n","    result = min(results['eval'], key=lambda x: x[1])\n","    id = results['eval'].index(result)\n","    best_result = {}\n","    best_result['params'] = results['params'][id]\n","    best_result['eval'] = results['eval'][id]\n","    best_result['checkpoints'] = results['checkpoints'][id]\n","\n","    return best_result"]},{"cell_type":"code","execution_count":215,"metadata":{},"outputs":[],"source":["train_size=0.99\n","epochs=3\n","batch_size=1024\n","learning_rate=0.01\n","lr_schedule=False\n","callback=True\n","grid_number=3\n","hyperparams = {\n","    'recency_days': [15],\n","    'weight_decay': [0.25],\n","    'neurals': [[64,16], [64,64,32], [64,64,32,32,16], [64,64,32,32,16,16], [256,64,64,32,32,16,16]],\n","    'num_outputs': [16],\n","    }"]},{"cell_type":"code","execution_count":216,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Loading clean data...\n","Calculating relative price...\n","End calculating relative price. Finished in 6.995s.\n","-------------------------------------------------------------\n","FEATURE ENGINEERING PROCESSING...\n","CREATING PRODUCT TABLE...\n","Basic features processing...\n","Interaction rates processing...\n","Create product table successfully. Finished in 31.950s.\n","-------------------------------------------------------------\n","CREATING USER TABLE...\n","Basic features processing...\n","Interaction features processing...\n","Interaction rates processing...\n","Check datatypes processing...\n","Create user table successfully. Finished in 47.957s.\n","-------------------------------------------------------------\n","CREATE USER-PRODUCT INTERACTION TABLE...\n","Calculating recency processing...\n","Calculating recency successfully. Finished in 1.290s.\n","-------------------------------------------------------------\n","Calculating basic interactions...\n","Calculating interaction scores...\n","Create interaction table successfully. Finished in 10.061s.\n","-------------------------------------------------------------\n","CREATE TRAINING TABLE...\n","Create training table successfully. Finished in 1.435s.\n","-------------------------------------------------------------\n","Extract features successfully.\n","Finished in 91.488s.\n","-------------------------------------------------------------\n","Running 3 random searching...\n","Fine Tuning Number 0, hyperparameters summary:\n","\u001b[31m{'recency_days': 15, 'weight_decay': 0.25, 'neurals': [64, 64, 32, 32, 16], 'num_outputs': 16}\u001b[0m\n","1. Train-Dev-Test plitting proceesing...\n","2. Bulding model...\n","Model: \"model_77\"\n","__________________________________________________________________________________________________\n"," Layer (type)                   Output Shape         Param #     Connected to                     \n","==================================================================================================\n"," input_222 (InputLayer)         [(None, 16)]         0           []                               \n","                                                                                                  \n"," input_223 (InputLayer)         [(None, 11)]         0           []                               \n","                                                                                                  \n"," sequential_165 (Sequential)    (None, 16)           10016       ['input_222[0][0]']              \n","                                                                                                  \n"," sequential_166 (Sequential)    (None, 16)           9676        ['input_223[0][0]']              \n","                                                                                                  \n"," tf.math.l2_normalize_154 (TFOp  (None, 16)          0           ['sequential_165[0][0]']         \n"," Lambda)                                                                                          \n","                                                                                                  \n"," tf.math.l2_normalize_155 (TFOp  (None, 16)          0           ['sequential_166[0][0]']         \n"," Lambda)                                                                                          \n","                                                                                                  \n"," dot_77 (Dot)                   (None, 1)            0           ['tf.math.l2_normalize_154[0][0]'\n","                                                                 , 'tf.math.l2_normalize_155[0][0]\n","                                                                 ']                               \n","                                                                                                  \n","==================================================================================================\n","Total params: 19,692\n","Trainable params: 18,870\n","Non-trainable params: 822\n","__________________________________________________________________________________________________\n","3. Training processing...\n","Epoch 1/3\n","1829/1829 [==============================] - ETA: 0s - loss: 0.0241 - root_mean_squared_error: 0.1552\n","Epoch 1: val_loss improved from inf to 0.02381, saving model to outputs\\checkpoints\\CBDeepLearning\\126\\1.ckpt\n","1829/1829 [==============================] - 31s 15ms/step - loss: 0.0241 - root_mean_squared_error: 0.1552 - val_loss: 0.0238 - val_root_mean_squared_error: 0.1543\n","Epoch 2/3\n","1827/1829 [============================>.] - ETA: 0s - loss: 0.0232 - root_mean_squared_error: 0.1522\n","Epoch 2: val_loss did not improve from 0.02381\n","1829/1829 [==============================] - 30s 16ms/step - loss: 0.0232 - root_mean_squared_error: 0.1522 - val_loss: 0.0305 - val_root_mean_squared_error: 0.1747\n","Epoch 3/3\n","1828/1829 [============================>.] - ETA: 0s - loss: 0.0229 - root_mean_squared_error: 0.1512\n","Epoch 3: val_loss did not improve from 0.02381\n","1829/1829 [==============================] - 28s 15ms/step - loss: 0.0229 - root_mean_squared_error: 0.1512 - val_loss: 0.0267 - val_root_mean_squared_error: 0.1634\n","4. Evaluation processing...\n","287/287 [==============================] - 4s 13ms/step - loss: 0.0250 - root_mean_squared_error: 0.1581\n","\u001b[31mEvalulation result (RMSE): [0.02499989978969097, 0.15811356902122498]\u001b[0m\n","Saving results...\n","\u001b[31mSaved to outputs//checkpoints//CBDeepLearning//126.\u001b[0m\n","\u001b[32m\n","Finished model 0 in 96.808s\n","\u001b[0m\n","-+--+--+--+--+--+--+--+--+--+--+--+--+--+--+--+--+--+--+--+--+--+--+--+--+-\n","Fine Tuning Number 1, hyperparameters summary:\n","\u001b[31m{'recency_days': 15, 'weight_decay': 0.25, 'neurals': [256, 64, 64, 32, 32, 16, 16], 'num_outputs': 16}\u001b[0m\n","1. Train-Dev-Test plitting proceesing...\n","2. Bulding model...\n","Model: \"model_78\"\n","__________________________________________________________________________________________________\n"," Layer (type)                   Output Shape         Param #     Connected to                     \n","==================================================================================================\n"," input_226 (InputLayer)         [(None, 16)]         0           []                               \n","                                                                                                  \n"," input_227 (InputLayer)         [(None, 11)]         0           []                               \n","                                                                                                  \n"," sequential_167 (Sequential)    (None, 16)           31088       ['input_226[0][0]']              \n","                                                                                                  \n"," sequential_168 (Sequential)    (None, 16)           29788       ['input_227[0][0]']              \n","                                                                                                  \n"," tf.math.l2_normalize_156 (TFOp  (None, 16)          0           ['sequential_167[0][0]']         \n"," Lambda)                                                                                          \n","                                                                                                  \n"," tf.math.l2_normalize_157 (TFOp  (None, 16)          0           ['sequential_168[0][0]']         \n"," Lambda)                                                                                          \n","                                                                                                  \n"," dot_78 (Dot)                   (None, 1)            0           ['tf.math.l2_normalize_156[0][0]'\n","                                                                 , 'tf.math.l2_normalize_157[0][0]\n","                                                                 ']                               \n","                                                                                                  \n","==================================================================================================\n","Total params: 60,876\n","Trainable params: 58,966\n","Non-trainable params: 1,910\n","__________________________________________________________________________________________________\n","3. Training processing...\n","Epoch 1/3\n","1828/1828 [==============================] - ETA: 0s - loss: 0.0240 - root_mean_squared_error: 0.1548\n","Epoch 1: val_loss improved from inf to 0.02689, saving model to outputs\\checkpoints\\CBDeepLearning\\127\\1.ckpt\n","1828/1828 [==============================] - 41s 21ms/step - loss: 0.0240 - root_mean_squared_error: 0.1548 - val_loss: 0.0269 - val_root_mean_squared_error: 0.1640\n","Epoch 2/3\n","1826/1828 [============================>.] - ETA: 0s - loss: 0.0231 - root_mean_squared_error: 0.1521\n","Epoch 2: val_loss did not improve from 0.02689\n","1828/1828 [==============================] - 42s 23ms/step - loss: 0.0231 - root_mean_squared_error: 0.1521 - val_loss: 0.0272 - val_root_mean_squared_error: 0.1650\n","Epoch 3/3\n","1826/1828 [============================>.] - ETA: 0s - loss: 0.0229 - root_mean_squared_error: 0.1514\n","Epoch 3: val_loss did not improve from 0.02689\n","1828/1828 [==============================] - 30s 17ms/step - loss: 0.0229 - root_mean_squared_error: 0.1514 - val_loss: 0.0290 - val_root_mean_squared_error: 0.1704\n","4. Evaluation processing...\n","298/298 [==============================] - 1s 4ms/step - loss: 0.0257 - root_mean_squared_error: 0.1602\n","\u001b[31mEvalulation result (RMSE): [0.025676380842924118, 0.16023850440979004]\u001b[0m\n","Saving results...\n","\u001b[31mSaved to outputs//checkpoints//CBDeepLearning//127.\u001b[0m\n","\u001b[32m\n","Finished model 1 in 120.591s\n","\u001b[0m\n","-+--+--+--+--+--+--+--+--+--+--+--+--+--+--+--+--+--+--+--+--+--+--+--+--+-\n","Fine Tuning Number 2, hyperparameters summary:\n","\u001b[31m{'recency_days': 15, 'weight_decay': 0.25, 'neurals': [64, 16], 'num_outputs': 16}\u001b[0m\n","1. Train-Dev-Test plitting proceesing...\n","2. Bulding model...\n","Model: \"model_79\"\n","__________________________________________________________________________________________________\n"," Layer (type)                   Output Shape         Param #     Connected to                     \n","==================================================================================================\n"," input_230 (InputLayer)         [(None, 16)]         0           []                               \n","                                                                                                  \n"," input_231 (InputLayer)         [(None, 11)]         0           []                               \n","                                                                                                  \n"," sequential_169 (Sequential)    (None, 16)           2720        ['input_230[0][0]']              \n","                                                                                                  \n"," sequential_170 (Sequential)    (None, 16)           2380        ['input_231[0][0]']              \n","                                                                                                  \n"," tf.math.l2_normalize_158 (TFOp  (None, 16)          0           ['sequential_169[0][0]']         \n"," Lambda)                                                                                          \n","                                                                                                  \n"," tf.math.l2_normalize_159 (TFOp  (None, 16)          0           ['sequential_170[0][0]']         \n"," Lambda)                                                                                          \n","                                                                                                  \n"," dot_79 (Dot)                   (None, 1)            0           ['tf.math.l2_normalize_158[0][0]'\n","                                                                 , 'tf.math.l2_normalize_159[0][0]\n","                                                                 ']                               \n","                                                                                                  \n","==================================================================================================\n","Total params: 5,100\n","Trainable params: 4,790\n","Non-trainable params: 310\n","__________________________________________________________________________________________________\n","3. Training processing...\n","Epoch 1/3\n","1829/1829 [==============================] - ETA: 0s - loss: 0.0241 - root_mean_squared_error: 0.1552\n","Epoch 1: val_loss improved from inf to 0.02532, saving model to outputs\\checkpoints\\CBDeepLearning\\128\\1.ckpt\n","1829/1829 [==============================] - 15s 7ms/step - loss: 0.0241 - root_mean_squared_error: 0.1552 - val_loss: 0.0253 - val_root_mean_squared_error: 0.1591\n","Epoch 2/3\n","1827/1829 [============================>.] - ETA: 0s - loss: 0.0232 - root_mean_squared_error: 0.1525\n","Epoch 2: val_loss did not improve from 0.02532\n","1829/1829 [==============================] - 14s 7ms/step - loss: 0.0232 - root_mean_squared_error: 0.1525 - val_loss: 0.0260 - val_root_mean_squared_error: 0.1613\n","Epoch 3/3\n","1826/1829 [============================>.] - ETA: 0s - loss: 0.0230 - root_mean_squared_error: 0.1515\n","Epoch 3: val_loss improved from 0.02532 to 0.02521, saving model to outputs\\checkpoints\\CBDeepLearning\\128\\3.ckpt\n","1829/1829 [==============================] - 17s 10ms/step - loss: 0.0230 - root_mean_squared_error: 0.1515 - val_loss: 0.0252 - val_root_mean_squared_error: 0.1588\n","4. Evaluation processing...\n","286/286 [==============================] - 1s 4ms/step - loss: 0.0280 - root_mean_squared_error: 0.1674\n","\u001b[31mEvalulation result (RMSE): [0.028014112263917923, 0.1673741638660431]\u001b[0m\n","Saving results...\n","\u001b[31mSaved to outputs//checkpoints//CBDeepLearning//128.\u001b[0m\n","\u001b[32m\n","Finished model 2 in 52.669s\n","\u001b[0m\n","-+--+--+--+--+--+--+--+--+--+--+--+--+--+--+--+--+--+--+--+--+--+--+--+--+-\n","\u001b[31mBest params: \u001b[0m\n","\u001b[31m{'recency_days': 15, 'weight_decay': 0.25, 'neurals': [64, 64, 32, 32, 16], 'num_outputs': 16}\u001b[0m\n","\u001b[31mLoss = [0.02499989978969097, 0.15811356902122498]; Checkpoint saved in outputs//checkpoints//CBDeepLearning//126\u001b[0m\n"]}],"source":["results_1 = ModelFineTuning(hyperparams, grid_number=grid_number, train_size=train_size, epochs=epochs, batch_size=batch_size,\n","                   learning_rate=learning_rate, lr_schedule=True, callback=True)"]},{"cell_type":"code","execution_count":217,"metadata":{},"outputs":[],"source":["grid_number=3\n","hyperparams = {\n","    'recency_days': [15],\n","    'weight_decay': [0.5],\n","    'neurals': [[64,64,32], [64,64,32,32,16]],\n","    'num_outputs': [16],\n","    }"]},{"cell_type":"code","execution_count":218,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Loading clean data...\n","Calculating relative price...\n","End calculating relative price. Finished in 7.646s.\n","-------------------------------------------------------------\n","FEATURE ENGINEERING PROCESSING...\n","CREATING PRODUCT TABLE...\n","Basic features processing...\n","Interaction rates processing...\n","Create product table successfully. Finished in 30.161s.\n","-------------------------------------------------------------\n","CREATING USER TABLE...\n","Basic features processing...\n","Interaction features processing...\n","Interaction rates processing...\n","Check datatypes processing...\n","Create user table successfully. Finished in 56.751s.\n","-------------------------------------------------------------\n","CREATE USER-PRODUCT INTERACTION TABLE...\n","Calculating recency processing...\n","Calculating recency successfully. Finished in 1.426s.\n","-------------------------------------------------------------\n","Calculating basic interactions...\n","Calculating interaction scores...\n","Create interaction table successfully. Finished in 8.625s.\n","-------------------------------------------------------------\n","CREATE TRAINING TABLE...\n","Create training table successfully. Finished in 1.835s.\n","-------------------------------------------------------------\n","Extract features successfully.\n","Finished in 97.462s.\n","-------------------------------------------------------------\n","Running 2 random searching...\n","Fine Tuning Number 0, hyperparameters summary:\n","\u001b[31m{'recency_days': 15, 'weight_decay': 0.5, 'neurals': [64, 64, 32, 32, 16], 'num_outputs': 16}\u001b[0m\n","1. Train-Dev-Test plitting proceesing...\n","2. Bulding model...\n","Model: \"model_80\"\n","__________________________________________________________________________________________________\n"," Layer (type)                   Output Shape         Param #     Connected to                     \n","==================================================================================================\n"," input_234 (InputLayer)         [(None, 16)]         0           []                               \n","                                                                                                  \n"," input_235 (InputLayer)         [(None, 11)]         0           []                               \n","                                                                                                  \n"," sequential_171 (Sequential)    (None, 16)           10016       ['input_234[0][0]']              \n","                                                                                                  \n"," sequential_172 (Sequential)    (None, 16)           9676        ['input_235[0][0]']              \n","                                                                                                  \n"," tf.math.l2_normalize_160 (TFOp  (None, 16)          0           ['sequential_171[0][0]']         \n"," Lambda)                                                                                          \n","                                                                                                  \n"," tf.math.l2_normalize_161 (TFOp  (None, 16)          0           ['sequential_172[0][0]']         \n"," Lambda)                                                                                          \n","                                                                                                  \n"," dot_80 (Dot)                   (None, 1)            0           ['tf.math.l2_normalize_160[0][0]'\n","                                                                 , 'tf.math.l2_normalize_161[0][0]\n","                                                                 ']                               \n","                                                                                                  \n","==================================================================================================\n","Total params: 19,692\n","Trainable params: 18,870\n","Non-trainable params: 822\n","__________________________________________________________________________________________________\n","3. Training processing...\n","Epoch 1/3\n","1826/1827 [============================>.] - ETA: 0s - loss: 0.0425 - root_mean_squared_error: 0.2061\n","Epoch 1: val_loss improved from inf to 0.05126, saving model to outputs\\checkpoints\\CBDeepLearning\\129\\1.ckpt\n","1827/1827 [==============================] - 26s 13ms/step - loss: 0.0425 - root_mean_squared_error: 0.2061 - val_loss: 0.0513 - val_root_mean_squared_error: 0.2264\n","Epoch 2/3\n","1175/1827 [==================>...........] - ETA: 11s - loss: 0.0407 - root_mean_squared_error: 0.2018"]},{"ename":"KeyboardInterrupt","evalue":"","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[1;32mIn[218], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m results_2 \u001b[39m=\u001b[39m ModelFineTuning(hyperparams, grid_number\u001b[39m=\u001b[39;49mgrid_number, train_size\u001b[39m=\u001b[39;49mtrain_size, epochs\u001b[39m=\u001b[39;49mepochs, batch_size\u001b[39m=\u001b[39;49mbatch_size,\n\u001b[0;32m      2\u001b[0m                    learning_rate\u001b[39m=\u001b[39;49mlearning_rate, lr_schedule\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, callback\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n","Cell \u001b[1;32mIn[213], line 58\u001b[0m, in \u001b[0;36mModelFineTuning\u001b[1;34m(hyperparams, grid_number, train_size, epochs, batch_size, learning_rate, lr_schedule, callback)\u001b[0m\n\u001b[0;32m     56\u001b[0m \u001b[39m# train model\u001b[39;00m\n\u001b[0;32m     57\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39m3. Training processing...\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m---> 58\u001b[0m hist \u001b[39m=\u001b[39m currentmodel\u001b[39m.\u001b[39;49mfit([train[user_attr], train[product_attr]], train[target],\n\u001b[0;32m     59\u001b[0m                 validation_data \u001b[39m=\u001b[39;49m [[dev[user_attr], dev[product_attr]], dev[target]], \n\u001b[0;32m     60\u001b[0m                 epochs\u001b[39m=\u001b[39;49mepochs, batch_size\u001b[39m=\u001b[39;49mbatch_size, verbose\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m, callbacks\u001b[39m=\u001b[39;49m[callbacks])\n\u001b[0;32m     62\u001b[0m \u001b[39m# predict\u001b[39;00m\n\u001b[0;32m     63\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39m4. Evaluation processing...\u001b[39m\u001b[39m'\u001b[39m)\n","File \u001b[1;32mc:\\Users\\USER\\Music\\PROGRAMING\\Projects\\ThreeRrific-DSTC-2023\\venv\\lib\\site-packages\\keras\\utils\\traceback_utils.py:64\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     62\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m     63\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 64\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m     65\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:  \u001b[39m# pylint: disable=broad-except\u001b[39;00m\n\u001b[0;32m     66\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n","File \u001b[1;32mc:\\Users\\USER\\Music\\PROGRAMING\\Projects\\ThreeRrific-DSTC-2023\\venv\\lib\\site-packages\\keras\\engine\\training.py:1384\u001b[0m, in \u001b[0;36mModel.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1377\u001b[0m \u001b[39mwith\u001b[39;00m tf\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mexperimental\u001b[39m.\u001b[39mTrace(\n\u001b[0;32m   1378\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[0;32m   1379\u001b[0m     epoch_num\u001b[39m=\u001b[39mepoch,\n\u001b[0;32m   1380\u001b[0m     step_num\u001b[39m=\u001b[39mstep,\n\u001b[0;32m   1381\u001b[0m     batch_size\u001b[39m=\u001b[39mbatch_size,\n\u001b[0;32m   1382\u001b[0m     _r\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m):\n\u001b[0;32m   1383\u001b[0m   callbacks\u001b[39m.\u001b[39mon_train_batch_begin(step)\n\u001b[1;32m-> 1384\u001b[0m   tmp_logs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain_function(iterator)\n\u001b[0;32m   1385\u001b[0m   \u001b[39mif\u001b[39;00m data_handler\u001b[39m.\u001b[39mshould_sync:\n\u001b[0;32m   1386\u001b[0m     context\u001b[39m.\u001b[39masync_wait()\n","File \u001b[1;32mc:\\Users\\USER\\Music\\PROGRAMING\\Projects\\ThreeRrific-DSTC-2023\\venv\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n","File \u001b[1;32mc:\\Users\\USER\\Music\\PROGRAMING\\Projects\\ThreeRrific-DSTC-2023\\venv\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:915\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    912\u001b[0m compiler \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mxla\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mnonXla\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    914\u001b[0m \u001b[39mwith\u001b[39;00m OptionalXlaContext(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 915\u001b[0m   result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)\n\u001b[0;32m    917\u001b[0m new_tracing_count \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    918\u001b[0m without_tracing \u001b[39m=\u001b[39m (tracing_count \u001b[39m==\u001b[39m new_tracing_count)\n","File \u001b[1;32mc:\\Users\\USER\\Music\\PROGRAMING\\Projects\\ThreeRrific-DSTC-2023\\venv\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:947\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    944\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n\u001b[0;32m    945\u001b[0m   \u001b[39m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[0;32m    946\u001b[0m   \u001b[39m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[1;32m--> 947\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_stateless_fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)  \u001b[39m# pylint: disable=not-callable\u001b[39;00m\n\u001b[0;32m    948\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_stateful_fn \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    949\u001b[0m   \u001b[39m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[0;32m    950\u001b[0m   \u001b[39m# in parallel.\u001b[39;00m\n\u001b[0;32m    951\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n","File \u001b[1;32mc:\\Users\\USER\\Music\\PROGRAMING\\Projects\\ThreeRrific-DSTC-2023\\venv\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:2956\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2953\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n\u001b[0;32m   2954\u001b[0m   (graph_function,\n\u001b[0;32m   2955\u001b[0m    filtered_flat_args) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[1;32m-> 2956\u001b[0m \u001b[39mreturn\u001b[39;00m graph_function\u001b[39m.\u001b[39;49m_call_flat(\n\u001b[0;32m   2957\u001b[0m     filtered_flat_args, captured_inputs\u001b[39m=\u001b[39;49mgraph_function\u001b[39m.\u001b[39;49mcaptured_inputs)\n","File \u001b[1;32mc:\\Users\\USER\\Music\\PROGRAMING\\Projects\\ThreeRrific-DSTC-2023\\venv\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:1853\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1849\u001b[0m possible_gradient_type \u001b[39m=\u001b[39m gradients_util\u001b[39m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   1850\u001b[0m \u001b[39mif\u001b[39;00m (possible_gradient_type \u001b[39m==\u001b[39m gradients_util\u001b[39m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   1851\u001b[0m     \u001b[39mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   1852\u001b[0m   \u001b[39m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> 1853\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_build_call_outputs(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_inference_function\u001b[39m.\u001b[39;49mcall(\n\u001b[0;32m   1854\u001b[0m       ctx, args, cancellation_manager\u001b[39m=\u001b[39;49mcancellation_manager))\n\u001b[0;32m   1855\u001b[0m forward_backward \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   1856\u001b[0m     args,\n\u001b[0;32m   1857\u001b[0m     possible_gradient_type,\n\u001b[0;32m   1858\u001b[0m     executing_eagerly)\n\u001b[0;32m   1859\u001b[0m forward_function, args_with_tangents \u001b[39m=\u001b[39m forward_backward\u001b[39m.\u001b[39mforward()\n","File \u001b[1;32mc:\\Users\\USER\\Music\\PROGRAMING\\Projects\\ThreeRrific-DSTC-2023\\venv\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:499\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    497\u001b[0m \u001b[39mwith\u001b[39;00m _InterpolateFunctionError(\u001b[39mself\u001b[39m):\n\u001b[0;32m    498\u001b[0m   \u001b[39mif\u001b[39;00m cancellation_manager \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 499\u001b[0m     outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39;49mexecute(\n\u001b[0;32m    500\u001b[0m         \u001b[39mstr\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msignature\u001b[39m.\u001b[39;49mname),\n\u001b[0;32m    501\u001b[0m         num_outputs\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_num_outputs,\n\u001b[0;32m    502\u001b[0m         inputs\u001b[39m=\u001b[39;49margs,\n\u001b[0;32m    503\u001b[0m         attrs\u001b[39m=\u001b[39;49mattrs,\n\u001b[0;32m    504\u001b[0m         ctx\u001b[39m=\u001b[39;49mctx)\n\u001b[0;32m    505\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    506\u001b[0m     outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m    507\u001b[0m         \u001b[39mstr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msignature\u001b[39m.\u001b[39mname),\n\u001b[0;32m    508\u001b[0m         num_outputs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    511\u001b[0m         ctx\u001b[39m=\u001b[39mctx,\n\u001b[0;32m    512\u001b[0m         cancellation_manager\u001b[39m=\u001b[39mcancellation_manager)\n","File \u001b[1;32mc:\\Users\\USER\\Music\\PROGRAMING\\Projects\\ThreeRrific-DSTC-2023\\venv\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py:54\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m   ctx\u001b[39m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 54\u001b[0m   tensors \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39;49mTFE_Py_Execute(ctx\u001b[39m.\u001b[39;49m_handle, device_name, op_name,\n\u001b[0;32m     55\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[0;32m     56\u001b[0m \u001b[39mexcept\u001b[39;00m core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     57\u001b[0m   \u001b[39mif\u001b[39;00m name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n","\u001b[1;31mKeyboardInterrupt\u001b[0m: "]}],"source":["results_2 = ModelFineTuning(hyperparams, grid_number=grid_number, train_size=train_size, epochs=epochs, batch_size=batch_size,\n","                   learning_rate=learning_rate, lr_schedule=True, callback=True)"]},{"cell_type":"code","execution_count":16,"metadata":{},"outputs":[],"source":["def save_model(model):\n","    print('Saving model...')\n","    path = 'outputs//models'\n","    if not os.path.exists(path):\n","        os.makedirs(path)\n","    model_ver = max([int(i.split('.')[0]) for i in os.listdir(path)]+[0]) + 1\n","    model.save(f'{path}//{model_ver}.keras')\n","    print(f\"Saved to '{path}//{model_ver}.keras'\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Load weights by lastest checkpoint\n","def load_latest_checkpoint(modelname):\n","    print('Loading latest checkpoint...')\n","    model = models[modelname]\n","    #model.summary()\n","\n","    path = f\"outputs//checkpoints//{modelname}\"\n","    if not os.path.exists(path):\n","        os.makedirs(path)\n","    model_ver = max([int(i.split('.')[0]) for i in os.listdir(path)]+[0])\n","\n","    if model_ver==0: print('There is no checkpoint')\n","    else: print('Found checkpoint ',model_ver)\n","\n","    latest = tf.train.latest_checkpoint(f\"{path}//{model_ver}\")\n","    model.load_weights(latest)\n","    print('Loaded: ',latest)\n","    return model\n","\n","def load_latest_model():\n","    print('Loading latest model...')\n","    path = f\"outputs//models\"\n","    if not os.path.exists(path):\n","        os.makedirs(path)\n","    model_ver = max([int(i.split('.')[0]) for i in os.listdir(path)]+[0])\n","\n","    if model_ver==0: print('There is no model')\n","    else: print('Found model ',model_ver)\n","\n","    return tf.keras.models.load_model(f'{path}//{model_ver}.keras')"]},{"cell_type":"code","execution_count":113,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Loading latest checkpoint...\n","Found checkpoint  27\n","Loaded:  outputs//checkpoints//CBDeepLearning//27\\2.ckpt\n","Saving model...\n","Saved to 'outputs//models//2.keras'\n"]}],"source":["model = load_latest_checkpoint('CBDeepLearning')\n","save_model(model)"]},{"cell_type":"code","execution_count":19,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Loading latest model...\n","Found model  2\n"]}],"source":["model = load_latest_model()"]},{"cell_type":"code","execution_count":20,"metadata":{},"outputs":[],"source":["def predict(model,input):\n","    output = model.predict(input)\n","    return output"]},{"cell_type":"code","execution_count":21,"metadata":{},"outputs":[],"source":["def user_content(user_id):\n","    user_content = userTable.loc[userTable.user_id==user_id]\n","    return user_content\n","\n","def product_content(product_id):\n","    product_content = productTable.loc[productTable.product_id==product_id]\n","    return product_content"]},{"cell_type":"code","execution_count":22,"metadata":{},"outputs":[],"source":["id = 5881337\n","proda = product_content(id)\n","usera = user_content(4661182)"]},{"cell_type":"code","execution_count":121,"metadata":{},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>user_id</th>\n","      <th>product_id</th>\n","      <th>interaction_score</th>\n","      <th>views_user</th>\n","      <th>carts_user</th>\n","      <th>remove_from_carts_user</th>\n","      <th>purchases_user</th>\n","      <th>avg_view_price</th>\n","      <th>avg_purchase_price</th>\n","      <th>avg_view_relative_price</th>\n","      <th>...</th>\n","      <th>avg_price</th>\n","      <th>relative_price</th>\n","      <th>views_product</th>\n","      <th>carts_product</th>\n","      <th>remove_from_carts_product</th>\n","      <th>purchases_product</th>\n","      <th>cart_per_view_product</th>\n","      <th>purchase_per_view_product</th>\n","      <th>remove_per_cart_product</th>\n","      <th>purchase_per_cart_product</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>567830</th>\n","      <td>4661182</td>\n","      <td>5805749</td>\n","      <td>0.125992</td>\n","      <td>3</td>\n","      <td>2</td>\n","      <td>2</td>\n","      <td>0</td>\n","      <td>21.536667</td>\n","      <td>19.37</td>\n","      <td>5.556445</td>\n","      <td>...</td>\n","      <td>19.37</td>\n","      <td>6.816327</td>\n","      <td>54</td>\n","      <td>38</td>\n","      <td>17</td>\n","      <td>9</td>\n","      <td>0.007094</td>\n","      <td>0.005425</td>\n","      <td>0.009254</td>\n","      <td>0.005425</td>\n","    </tr>\n","    <tr>\n","      <th>407479</th>\n","      <td>4661182</td>\n","      <td>4185</td>\n","      <td>0.197926</td>\n","      <td>3</td>\n","      <td>2</td>\n","      <td>2</td>\n","      <td>0</td>\n","      <td>21.536667</td>\n","      <td>19.37</td>\n","      <td>5.556445</td>\n","      <td>...</td>\n","      <td>19.37</td>\n","      <td>6.816327</td>\n","      <td>744</td>\n","      <td>311</td>\n","      <td>97</td>\n","      <td>98</td>\n","      <td>0.058055</td>\n","      <td>0.059072</td>\n","      <td>0.052803</td>\n","      <td>0.059072</td>\n","    </tr>\n","    <tr>\n","      <th>942157</th>\n","      <td>4661182</td>\n","      <td>5778934</td>\n","      <td>0.065975</td>\n","      <td>3</td>\n","      <td>2</td>\n","      <td>2</td>\n","      <td>0</td>\n","      <td>21.536667</td>\n","      <td>19.37</td>\n","      <td>5.556445</td>\n","      <td>...</td>\n","      <td>25.87</td>\n","      <td>3.036683</td>\n","      <td>218</td>\n","      <td>19</td>\n","      <td>8</td>\n","      <td>3</td>\n","      <td>0.003547</td>\n","      <td>0.001808</td>\n","      <td>0.004355</td>\n","      <td>0.001808</td>\n","    </tr>\n","    <tr>\n","      <th>1816305</th>\n","      <td>4661182</td>\n","      <td>5667096</td>\n","      <td>0.000000</td>\n","      <td>3</td>\n","      <td>2</td>\n","      <td>2</td>\n","      <td>0</td>\n","      <td>21.536667</td>\n","      <td>19.37</td>\n","      <td>5.556445</td>\n","      <td>...</td>\n","      <td>3.81</td>\n","      <td>0.294991</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>4 rows × 30 columns</p>\n","</div>"],"text/plain":["         user_id  product_id  interaction_score  views_user  carts_user  \\\n","567830   4661182     5805749           0.125992           3           2   \n","407479   4661182        4185           0.197926           3           2   \n","942157   4661182     5778934           0.065975           3           2   \n","1816305  4661182     5667096           0.000000           3           2   \n","\n","         remove_from_carts_user  purchases_user  avg_view_price  \\\n","567830                        2               0       21.536667   \n","407479                        2               0       21.536667   \n","942157                        2               0       21.536667   \n","1816305                       2               0       21.536667   \n","\n","         avg_purchase_price  avg_view_relative_price  ...  avg_price  \\\n","567830                19.37                 5.556445  ...      19.37   \n","407479                19.37                 5.556445  ...      19.37   \n","942157                19.37                 5.556445  ...      25.87   \n","1816305               19.37                 5.556445  ...       3.81   \n","\n","         relative_price  views_product  carts_product  \\\n","567830         6.816327             54             38   \n","407479         6.816327            744            311   \n","942157         3.036683            218             19   \n","1816305        0.294991              1              0   \n","\n","         remove_from_carts_product  purchases_product  cart_per_view_product  \\\n","567830                          17                  9               0.007094   \n","407479                          97                 98               0.058055   \n","942157                           8                  3               0.003547   \n","1816305                          1                  0               0.000000   \n","\n","         purchase_per_view_product  remove_per_cart_product  \\\n","567830                    0.005425                 0.009254   \n","407479                    0.059072                 0.052803   \n","942157                    0.001808                 0.004355   \n","1816305                   0.000000                 0.000000   \n","\n","         purchase_per_cart_product  \n","567830                    0.005425  \n","407479                    0.059072  \n","942157                    0.001808  \n","1816305                   0.000000  \n","\n","[4 rows x 30 columns]"]},"execution_count":121,"metadata":{},"output_type":"execute_result"}],"source":["df.loc[(df.user_id==4661182)]"]},{"cell_type":"code","execution_count":23,"metadata":{},"outputs":[],"source":["def userNN(model):\n","    # use model\n","    user_layer = model.layers[2]\n","    input_user = Input(shape=user_layer.input_shape[1:])\n","    output_user = user_layer(input_user)\n","    user_model = Model(inputs=input_user, outputs=output_user)\n","\n","    return user_model"]},{"cell_type":"code","execution_count":24,"metadata":{},"outputs":[],"source":["def productNN(model):\n","    # product model\n","    product_layer = model.layers[3]\n","    input_product = Input(shape=product_layer.input_shape[1:])\n","    output_product = product_layer(input_product)\n","    product_model = Model(inputs=input_product, outputs=output_product)\n","\n","    return product_model"]},{"cell_type":"code","execution_count":31,"metadata":{},"outputs":[],"source":["def productVector(model):\n","    product_model = productNN(model)\n","    productVectors = pd.DataFrame(columns=range(num_outputs+1))\n","    productVectors = productVectors.rename(columns={0: 'product_id'})\n","    print('Found {} products'.format(len(productTable.product_id)))\n","    for id, product_id in enumerate(productTable.product_id):\n","        print('Processing {}: product id {}...'.format(id, product_id))\n","        product = product_content(product_id)\n","        new_row = [product_id]\n","        new_row = new_row + list(product_model.predict(product.iloc[:,3:])[0])\n","        productVectors.loc[len(df)] = new_row\n","\n","    return productVectors"]},{"cell_type":"code","execution_count":24,"metadata":{},"outputs":[],"source":["def recommend_list(user_ids,model,top):\n","    RecList = {}\n","    for user_id in user_ids:\n","        user = user_content(id)\n","        currentuser = df.loc[df.user_id==user_id]\n","        RecList[user_id] = pd.DataFrame\n","        for _, row in productTable.iloc[:,:].iterrows():\n","            product_id = row.product_id\n","            product = product_content(product_id)\n","            exists = ((currentuser['product_id'] == product_id) & (currentuser['user_id'] == user_id)).any()\n","            if not exists:\n","                prediction = predict(model, [user.iloc[:,3:],product.iloc[:,3:]])\n","                if len(RecList[user_id])==top:\n","                    min_id = np.argmin(RecList[user_id])\n","                    if RecList[user_id][min_id] < prediction:\n","                        RecList[user_id][min_id] = product_id\n","                else: RecList[user_id] = RecList[user_id] + [product_id]\n","    return RecList"]},{"cell_type":"code","execution_count":236,"metadata":{},"outputs":[{"ename":"ValueError","evalue":"Data cardinality is ambiguous:\n  x sizes: 0, 1\nMake sure all arrays contain the same number of samples.","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)","Cell \u001b[1;32mIn[236], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m RecList \u001b[39m=\u001b[39m recommend_list(user_ids\u001b[39m=\u001b[39;49m[\u001b[39m1180452\u001b[39;49m], model\u001b[39m=\u001b[39;49mmodel, top\u001b[39m=\u001b[39;49m\u001b[39m10\u001b[39;49m)\n","Cell \u001b[1;32mIn[235], line 12\u001b[0m, in \u001b[0;36mrecommend_list\u001b[1;34m(user_ids, model, top)\u001b[0m\n\u001b[0;32m     10\u001b[0m exists \u001b[39m=\u001b[39m ((currentuser[\u001b[39m'\u001b[39m\u001b[39mproduct_id\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m==\u001b[39m product_id) \u001b[39m&\u001b[39m (currentuser[\u001b[39m'\u001b[39m\u001b[39muser_id\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m==\u001b[39m user_id))\u001b[39m.\u001b[39many()\n\u001b[0;32m     11\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m exists:\n\u001b[1;32m---> 12\u001b[0m     prediction \u001b[39m=\u001b[39m predict(model, [user\u001b[39m.\u001b[39;49miloc[:,\u001b[39m3\u001b[39;49m:],product\u001b[39m.\u001b[39;49miloc[:,\u001b[39m3\u001b[39;49m:]])\n\u001b[0;32m     13\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(RecList[user_id])\u001b[39m==\u001b[39mtop:\n\u001b[0;32m     14\u001b[0m         min_id \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39margmin(RecList[user_id])\n","Cell \u001b[1;32mIn[58], line 2\u001b[0m, in \u001b[0;36mpredict\u001b[1;34m(model, input)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mpredict\u001b[39m(model,\u001b[39minput\u001b[39m):\n\u001b[1;32m----> 2\u001b[0m     output \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mpredict(\u001b[39minput\u001b[39;49m)\n\u001b[0;32m      3\u001b[0m     \u001b[39mreturn\u001b[39;00m output\n","File \u001b[1;32mc:\\Users\\USER\\Music\\PROGRAMING\\Projects\\ThreeRrific-DSTC-2023\\venv\\lib\\site-packages\\keras\\utils\\traceback_utils.py:67\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     65\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:  \u001b[39m# pylint: disable=broad-except\u001b[39;00m\n\u001b[0;32m     66\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[1;32m---> 67\u001b[0m   \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m     68\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m     69\u001b[0m   \u001b[39mdel\u001b[39;00m filtered_tb\n","File \u001b[1;32mc:\\Users\\USER\\Music\\PROGRAMING\\Projects\\ThreeRrific-DSTC-2023\\venv\\lib\\site-packages\\keras\\engine\\data_adapter.py:1653\u001b[0m, in \u001b[0;36m_check_data_cardinality\u001b[1;34m(data)\u001b[0m\n\u001b[0;32m   1649\u001b[0m   msg \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m  \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m sizes: \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[0;32m   1650\u001b[0m       label, \u001b[39m\"\u001b[39m\u001b[39m, \u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mjoin(\u001b[39mstr\u001b[39m(i\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m])\n\u001b[0;32m   1651\u001b[0m                        \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m tf\u001b[39m.\u001b[39mnest\u001b[39m.\u001b[39mflatten(single_data)))\n\u001b[0;32m   1652\u001b[0m msg \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mMake sure all arrays contain the same number of samples.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m-> 1653\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(msg)\n","\u001b[1;31mValueError\u001b[0m: Data cardinality is ambiguous:\n  x sizes: 0, 1\nMake sure all arrays contain the same number of samples."]}],"source":["RecList = recommend_list(user_ids=[1180452], model=model, top=10)"]},{"cell_type":"code","execution_count":233,"metadata":{},"outputs":[{"data":{"text/plain":["{1180452: []}"]},"execution_count":233,"metadata":{},"output_type":"execute_result"}],"source":["RecList"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.0"}},"nbformat":4,"nbformat_minor":4}
